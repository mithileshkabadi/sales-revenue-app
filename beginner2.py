# -*- coding: utf-8 -*-
"""Beginner2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Yz6LA1a7BeegZcEyDmjXWjvuoNcFtWZ
"""

import pandas as pd

def clean_data(file_path, save_path="cleaned_e_commerce_sales_data.csv"):
    """Loads, cleans, and processes the e-commerce dataset."""

    df = pd.read_csv(file_path)

    # Convert 'Order Date' to datetime format
    df['Order Date'] = pd.to_datetime(df['Order Date'], errors='coerce')

    # Fill missing values
    df['Region'] = df['Region'].fillna("Unknown")
    df['Age'] = df['Age'].fillna(df['Age'].median())
    df['Shipping Status'] = df['Shipping Status'].fillna("Unknown")

    # Create new time-based features
    df['Year'] = df['Order Date'].dt.year
    df['Month'] = df['Order Date'].dt.month
    df['Day'] = df['Order Date'].dt.day
    df['Weekday'] = df['Order Date'].dt.day_name()

    # Ensure Total Revenue is correctly calculated
    df['Total Revenue'] = df['Quantity'] * df['Unit Price']

    # Sort dataset by Order Date for time-series analysis
    df = df.sort_values(by='Order Date')

    # Save cleaned dataset
    df.to_csv(save_path, index=False)
    print(f"âœ… Data cleaned and saved as: {save_path}")

    return df

# Run cleaning function
clean_data("realistic_e_commerce_sales_data.csv")

"""Data Preprocessing"""

from sklearn.preprocessing import MinMaxScaler, LabelEncoder
import pandas as pd

def preprocess_data(file_path, save_path="preprocessed_e_commerce_sales_data.csv"):
    """Handles outliers, normalizes features, encodes categorical data, and aggregates data for forecasting."""

    df = pd.read_csv(file_path)

    # Convert 'Order Date' to datetime format
    df['Order Date'] = pd.to_datetime(df['Order Date'], errors='coerce')

    # Extract time-based features
    df['Year'] = df['Order Date'].dt.year
    df['Month'] = df['Order Date'].dt.month
    df['Day'] = df['Order Date'].dt.day
    df['Weekday'] = df['Order Date'].dt.day_name()
    df['Quarter'] = df['Order Date'].dt.quarter

    # Function to remove outliers using IQR
    def remove_outliers(df, column):
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

    # Remove outliers in Quantity and Total Revenue
    df = remove_outliers(df, 'Quantity')
    df = remove_outliers(df, 'Total Revenue')

    # Convert Total Revenue into thousands ($K) or millions ($M) for better readability
    df['Total Revenue ($K)'] = df['Total Revenue'] * 1000  # Convert back if originally scaled
    df['Total Revenue ($M)'] = df['Total Revenue ($K)'] / 1000  # Convert to millions

    # Normalize numerical features
    scaler = MinMaxScaler()
    df[['Unit Price', 'Quantity', 'Total Revenue', 'Shipping Fee']] = scaler.fit_transform(
        df[['Unit Price', 'Quantity', 'Total Revenue', 'Shipping Fee']]
    )

    # Encode categorical features while preserving interpretability
    encoder = LabelEncoder()
    df['Category Encoded'] = encoder.fit_transform(df['Category'])
    df['Region Encoded'] = encoder.fit_transform(df['Region'])
    df['Shipping Status Encoded'] = encoder.fit_transform(df['Shipping Status'])

    # Group by Category, Region, and Order Date to get aggregated revenue and quantity
    df_grouped = df.groupby(['Order Date', 'Category', 'Region']).agg({
        'Total Revenue ($K)': 'sum',
        'Total Revenue ($M)': 'sum',
        'Quantity': 'sum'
    }).reset_index()

    # Save preprocessed dataset
    df_grouped.to_csv(save_path, index=False)
    print(f"âœ… Preprocessing complete. Data saved as: {save_path}")

    return df_grouped

# Run preprocessing function
preprocess_data("cleaned_e_commerce_sales_data.csv")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_data(file_path):
    """Generates time-series trends, revenue distribution, and correlation heatmaps."""

    df_eda = pd.read_csv(file_path)
    df_eda['Order Date'] = pd.to_datetime(df_eda['Order Date'])

    # Ensure only numeric columns are used for calculations
    numeric_columns = df_eda.select_dtypes(include=['number']).columns
    df_numeric = df_eda[numeric_columns]

    # Check if 'Total Revenue' exists, else use 'Total Revenue ($K)'
    revenue_column = 'Total Revenue' if 'Total Revenue' in df_numeric.columns else 'Total Revenue ($K)'

    # Time-Series Trend of Total Revenue
    plt.figure(figsize=(12, 6))
    plt.plot(df_eda['Order Date'], df_numeric[revenue_column], marker='o', linestyle='-', color='b')
    plt.xlabel("Date")
    plt.ylabel("Total Revenue ($)")
    plt.title("Time-Series Trend of Total Revenue")
    plt.xticks(rotation=45)
    plt.grid()
    plt.show()

    # Distribution of Total Revenue
    plt.figure(figsize=(10, 5))
    sns.histplot(df_numeric[revenue_column], bins=30, kde=True, color='blue')
    plt.xlabel("Total Revenue ($)")
    plt.ylabel("Frequency")
    plt.title("Distribution of Total Revenue")
    plt.show()

    # Correlation Heatmap (Only for Numerical Columns)
    plt.figure(figsize=(8, 5))
    sns.heatmap(df_numeric.corr(), annot=True, cmap="coolwarm", fmt=".2f")
    plt.title("Correlation Heatmap")
    plt.show()

# Run visualization function
visualize_data("preprocessed_e_commerce_sales_data.csv")

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load datasets
df_original = pd.read_csv("cleaned_e_commerce_sales_data.csv")  # Update path if needed
df = pd.read_csv("preprocessed_e_commerce_sales_data.csv")  # Update path if needed

# Convert 'Order Date' to datetime format in both datasets before merging
df_original['Order Date'] = pd.to_datetime(df_original['Order Date'], errors='coerce')
df['Order Date'] = pd.to_datetime(df['Order Date'], errors='coerce')

# DEBUG: Print available columns before merging
print("âœ… Columns in df_original:", df_original.columns)
print("âœ… Columns in df:", df.columns)

# Merge back the 'Category' column using 'Order Date'
df_merged = df.merge(df_original[['Order Date', 'Category']], on='Order Date', how='left')

# DEBUG: Check if 'Total Revenue' exists after merging
print("âœ… Columns in df_merged after merging:", df_merged.columns)

if 'Total Revenue' not in df_merged.columns:
    print("âŒ ERROR: 'Total Revenue' column not found. Available columns:", df_merged.columns)
else:
    print("âœ… 'Total Revenue' column found. Proceeding with analysis.")

    # Step 3: Aggregate Sales Data by Category and Date
    df_category_trends = df_merged.groupby(['Order Date', 'Category'])['Total Revenue'].sum().reset_index()

    # Step 4: Pivot Data for Visualization
    category_pivot = df_category_trends.pivot(index='Order Date', columns='Category', values='Total Revenue')

    # Step 5: Plot Seasonal Trends for Different Categories
    plt.figure(figsize=(14, 7))
    for category in category_pivot.columns:
        plt.plot(category_pivot.index, category_pivot[category], label=f'{category}')

    plt.xlabel("Date")
    plt.ylabel("Total Revenue ($)")
    plt.title("ðŸ“Š Seasonal Trends in Product Categories")
    plt.legend(title="Product Category", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.xticks(rotation=45)
    plt.grid()
    plt.show()

    # Step 6: Display Summary Statistics
    print("\nðŸ“Š Summary Statistics of Category-Wise Trends:")
    print(df_category_trends.describe())

    # Save cleaned category trend data
    df_category_trends.to_csv("category_seasonality_trends.csv", index=False)
    print("âœ… Data saved as 'category_seasonality_trends.csv'")

# Load Dataset
file_path = "preprocessed_e_commerce_sales_data.csv"
df = pd.read_csv(file_path)

# Convert 'Order Date' to datetime format
df['Order Date'] = pd.to_datetime(df['Order Date'], errors='coerce')

# Step 1: Identify Revenue Spikes (Using Moving Average)
df['Rolling_Mean'] = df['Total Revenue ($K)'].rolling(window=7).mean()  # 7-day moving average
df['Revenue Spike'] = df['Total Revenue ($K)'] > (df['Rolling_Mean'] * 1.5)  # Mark spikes where revenue is 50% above avg

# Step 2: Compare Sales During High vs Normal Periods
sales_analysis = df.groupby('Revenue Spike').agg({
    'Total Revenue ($K)': ['mean', 'sum'],
    'Quantity': ['mean', 'sum']
}).reset_index()

print("\nðŸ“Š Sales Comparison - High vs Normal Sales Periods:")
print(sales_analysis)

# Step 3: Visualize Revenue Trends & Detected Spikes
plt.figure(figsize=(14, 6))
sns.lineplot(data=df, x="Order Date", y="Total Revenue ($K)", label="Total Revenue", color="blue")
sns.scatterplot(data=df[df['Revenue Spike']], x="Order Date", y="Total Revenue ($K)", color="red", label="Spikes")
plt.xlabel("Order Date")
plt.ylabel("Total Revenue ($K)")
plt.title("Sales Trend with Detected Spikes")
plt.xticks(rotation=45)
plt.legend()
plt.grid()
plt.show()

# Step 4: Seasonal Sales Trends (Monthly Revenue)
# Ensure 'Month' is in a valid format
df['Month'] = df['Order Date'].dt.strftime('%Y-%m')  # Converts to 'YYYY-MM' string format

# Aggregate Monthly Sales
monthly_sales = df.groupby('Month')['Total Revenue ($K)'].sum().reset_index()

# Convert 'Month' back to datetime for proper plotting
monthly_sales['Month'] = pd.to_datetime(monthly_sales['Month'])

# Plot Monthly Sales Trend
plt.figure(figsize=(14, 6))
sns.lineplot(data=monthly_sales, x="Month", y="Total Revenue ($K)", marker="o", color="green")
plt.xlabel("Month")
plt.ylabel("Total Revenue ($K)")
plt.title("Monthly Sales Trend")
plt.xticks(rotation=45)
plt.grid()
plt.show()

print("âœ… Alternative Promotion & External Factor Impact Analysis Completed.")

# Import required libraries
import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# Load dataset
file_path = "preprocessed_e_commerce_sales_data.csv"
df = pd.read_csv(file_path)

# Convert 'Order Date' to datetime format
df['Order Date'] = pd.to_datetime(df['Order Date'], errors='coerce')

# 1ï¸âƒ£ Extract Time-Based Features
df['Year'] = df['Order Date'].dt.year
df['Month'] = df['Order Date'].dt.month
df['Day'] = df['Order Date'].dt.day
df['Weekday'] = df['Order Date'].dt.day_name()
df['Quarter'] = df['Order Date'].dt.quarter
df['Is Weekend'] = df['Weekday'].isin(['Saturday', 'Sunday']).astype(int)

# 2ï¸âƒ£ Create Lagged Features (Previous Month Sales for Trend Analysis)
df['Prev Month Revenue'] = df['Total Revenue ($K)'].shift(30)

# 3ï¸âƒ£ Compute Rolling Statistics (Moving Averages for Smoothing)
df['Rolling Mean Revenue'] = df['Total Revenue ($K)'].rolling(window=7).mean()
df['Rolling Std Revenue'] = df['Total Revenue ($K)'].rolling(window=7).std()

# 4ï¸âƒ£ Encode Categorical Features
label_encoder = LabelEncoder()
df['Category Encoded'] = label_encoder.fit_transform(df['Category'])
df['Region Encoded'] = label_encoder.fit_transform(df['Region'])

# One-Hot Encoding for 'Category' and 'Region'
df = pd.get_dummies(df, columns=['Category', 'Region'], drop_first=True)

# Save the feature-engineered dataset
df.to_csv("feature_engineered_data.csv", index=False)

# Re-load necessary libraries since execution state was reset
import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# Re-upload file if necessary
file_path = "preprocessed_e_commerce_sales_data.csv"

# Load dataset
df = pd.read_csv(file_path)

# Convert 'Order Date' to datetime format
df['Order Date'] = pd.to_datetime(df['Order Date'], errors='coerce')

# 1ï¸âƒ£ Extract Time-Based Features
df['Year'] = df['Order Date'].dt.year
df['Month'] = df['Order Date'].dt.month
df['Day'] = df['Order Date'].dt.day
df['Weekday'] = df['Order Date'].dt.day_name()
df['Quarter'] = df['Order Date'].dt.quarter
df['Is Weekend'] = df['Weekday'].isin(['Saturday', 'Sunday']).astype(int)

# 2ï¸âƒ£ Create Lagged Features (Previous Month Sales for Trend Analysis)
df['Prev Month Revenue'] = df['Total Revenue ($K)'].shift(30)

# 3ï¸âƒ£ Compute Rolling Statistics (Moving Averages for Smoothing)
df['Rolling Mean Revenue'] = df['Total Revenue ($K)'].rolling(window=7).mean()
df['Rolling Std Revenue'] = df['Total Revenue ($K)'].rolling(window=7).std()

# 4ï¸âƒ£ Encode Categorical Features
label_encoder = LabelEncoder()
df['Category Encoded'] = label_encoder.fit_transform(df['Category'])
df['Region Encoded'] = label_encoder.fit_transform(df['Region'])

# One-Hot Encoding for 'Category' and 'Region'
df = pd.get_dummies(df, columns=['Category', 'Region'], drop_first=True)

# Save the feature-engineered dataset
output_path = "feature_engineered_data.csv"
df.to_csv(output_path, index=False)

"""Data Visualization for Feature file"""

# Load the feature-engineered dataset for visualization and analysis
df = pd.read_csv("feature_engineered_data.csv")

# Import necessary libraries for visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Set plot style
sns.set_theme(style="whitegrid")

# 1ï¸âƒ£ Visualizing Rolling Mean Revenue Over Time
plt.figure(figsize=(14, 6))
plt.plot(df['Order Date'], df['Rolling Mean Revenue'], label="7-Day Rolling Mean", color="blue")
plt.xlabel("Order Date")
plt.ylabel("Rolling Mean Revenue ($K)")
plt.title("ðŸ“Š 7-Day Rolling Mean of Revenue Over Time")
plt.xticks(rotation=45)
plt.legend()
plt.grid()
plt.show()

# 2ï¸âƒ£ Visualizing Rolling Standard Deviation of Revenue
plt.figure(figsize=(14, 6))
plt.plot(df['Order Date'], df['Rolling Std Revenue'], label="7-Day Rolling Std Dev", color="red")
plt.xlabel("Order Date")
plt.ylabel("Rolling Standard Deviation ($K)")
plt.title("ðŸ“Š 7-Day Rolling Standard Deviation of Revenue")
plt.xticks(rotation=45)
plt.legend()
plt.grid()
plt.show()

# 3ï¸âƒ£ Correlation Heatmap of Engineered Features
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("ðŸ“Š Correlation Heatmap of Feature-Engineered Data")
plt.show()

# 4ï¸âƒ£ Distribution of Total Revenue After Feature Engineering
plt.figure(figsize=(10, 5))
sns.histplot(df['Total Revenue ($K)'], bins=30, kde=True, color='green')
plt.xlabel("Total Revenue ($K)")
plt.ylabel("Frequency")
plt.title("ðŸ“Š Distribution of Total Revenue After Feature Engineering")
plt.show()

"""PCA Generation"""

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Load the feature-engineered dataset
file_path = "SalesData.csv"
df = pd.read_csv(file_path)

# Convert 'Order Date' to datetime format (if not already converted)
df['Order Date'] = pd.to_datetime(df['Order Date'], errors='coerce')

# Select only numeric columns for correlation analysis
numeric_df = df.select_dtypes(include=['number'])

# 1ï¸âƒ£ Generate the correlation heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(numeric_df.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap of Feature-Engineered Data")
plt.show()

# 2ï¸âƒ£ Standardize the data for PCA
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_df.dropna())  # Removing NaN values if any

# 3ï¸âƒ£ Apply PCA for Dimensionality Reduction
pca = PCA(n_components=5)  # Choosing top 5 principal components
pca_result = pca.fit_transform(scaled_data)

# Create a DataFrame with the PCA results
pca_df = pd.DataFrame(pca_result, columns=[f'PC{i+1}' for i in range(5)])

# 4ï¸âƒ£ Visualizing Explained Variance Ratio
plt.figure(figsize=(10, 6))
plt.plot(range(1, 6), pca.explained_variance_ratio_, marker='o', linestyle='--', color='b') # Changed range to (1, 11)
plt.xlabel("Principal Component")
plt.ylabel("Explained Variance Ratio")
plt.title("Explained Variance Ratio of Principal Components")
plt.grid()
plt.show()

# Save the PCA-transformed dataset
pca_output_path = "pca_feature_engineered_data.csv"
pca_df.to_csv(pca_output_path, index=False)

"""PCA Data Visualisation"""

sns.set_theme(style="whitegrid")

plt.figure(figsize=(10, 5))
sns.boxplot(data=pca_df)
plt.title("Distribution of Principal Components (PCA Transformed Data)")
plt.xlabel("Principal Components")
plt.ylabel("Values")
plt.xticks(rotation=45)
plt.grid()
plt.show()

explained_variance = [f'PC{i+1}' for i in range(len(pca_df.columns))]

plt.figure(figsize=(8, 5))
plt.bar(explained_variance, pca_df.var(), color='blue', alpha=0.7)
plt.xlabel("Principal Components")
plt.ylabel("Variance Explained")
plt.title("Variance Explained by Each Principal Component")
plt.xticks(rotation=45)
plt.grid()
plt.show()

import numpy as np
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(pca_df.columns) + 1), np.cumsum(pca_df.var() / np.sum(pca_df.var())), marker='o', linestyle='--', color='b')
plt.xlabel("Number of Principal Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("ðŸ“Š Cumulative Explained Variance by PCA Components")
plt.grid()
plt.show()

"""Step 7 : Model Training"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import numpy as np

# Load dataset
file_path = "feature_engineered_data.csv"
df = pd.read_csv(file_path)

# Convert 'Order Date' to datetime format
df['Order Date'] = pd.to_datetime(df['Order Date'], errors='coerce')

# Drop unnecessary columns if they exist
columns_to_drop = ["Order Date", "Weekday", "Total Revenue ($M)"]
df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])

# Fill missing values with mean
df.fillna(df.mean(), inplace=True)

# Define features and target variable
X = df.drop(columns=["Total Revenue ($K)"])
y = df["Total Revenue ($K)"]

# Train-test split (80-20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
import joblib

# Load Dataset
file_path = "SalesData.csv"
df = pd.read_csv(file_path)

# Convert 'Order Date' to datetime
df['Order Date'] = pd.to_datetime(df['Order Date'], errors='coerce')

# Drop unnecessary columns (like Customer ID, which cannot be used for predictions)
df.drop(columns=['Customer ID'], inplace=True, errors='ignore')

# Fill Missing Values
df.fillna({
    'Region': 'Unknown',
    'Shipping Status': 'Unknown',
    'Age': df['Age'].median()
}, inplace=True)

# Generate Time-based Features
df['Year'] = df['Order Date'].dt.year
df['Month'] = df['Order Date'].dt.month
df['Day'] = df['Order Date'].dt.day
df['Weekday'] = df['Order Date'].dt.day_name()
df['Quarter'] = df['Order Date'].dt.quarter
df['Is Weekend'] = df['Weekday'].isin(['Saturday', 'Sunday']).astype(int)

# Calculate Revenue
df['Total Revenue'] = df['Quantity'] * df['Unit Price']

# Remove Outliers
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

df = remove_outliers(df, 'Quantity')
df = remove_outliers(df, 'Total Revenue')

# Normalize Numerical Columns
scaler = MinMaxScaler()
numeric_cols = ['Unit Price', 'Quantity', 'Total Revenue']
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

# Encode Categorical Columns
label_encoder = LabelEncoder()
for col in ['Category', 'Region', 'Shipping Status']:  # List all categorical columns
    df[col] = label_encoder.fit_transform(df[col])

# Create Lag Features for Time-Series
df['Prev Month Revenue'] = df['Total Revenue'].shift(30)
df['Rolling Mean Revenue'] = df['Total Revenue'].rolling(window=7).mean()
df['Rolling Std Revenue'] = df['Total Revenue'].rolling(window=7).std()

# Drop rows with NaN values created by shifting
df.dropna(inplace=True)

# Define Features and Target Variable
X = df.drop(columns=['Order Date', 'Weekday', 'Total Revenue'])
y = df['Total Revenue']

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ensure only numeric values in training data
X_train = X_train.select_dtypes(include=[np.number])
X_test = X_test.select_dtypes(include=[np.number])

# Train Linear Regression Model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)

# Evaluate Linear Regression
mse_lr = mean_squared_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)
print(f"Linear Regression - MSE: {mse_lr}, RÂ²: {r2_lr}")

# Train Random Forest Model
rf_model = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_split=5, min_samples_leaf=2, random_state=42, n_jobs=-1)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Evaluate Random Forest
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)
print(f"Random Forest - MSE: {mse_rf}, RÂ²: {r2_rf}")

# Train XGBoost Model
xgb_model = XGBRegressor(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=5,
    reg_lambda=5,
    reg_alpha=3,
    random_state=42
)

xgb_model.fit(X_train, np.log1p(y_train))  # Log transform target
y_pred_xgb = np.expm1(xgb_model.predict(X_test))  # Reverse log transform

# Evaluate XGBoost
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)
print(f"XGBoost - MSE: {mse_xgb}, RÂ²: {r2_xgb}")

# Save Best Model
joblib.dump(xgb_model, "xgboost_sales_model.pkl")

# Feature Importance Visualization
feature_importance = xgb_model.feature_importances_
sorted_idx = np.argsort(feature_importance)[::-1]
sorted_features = X_train.columns[sorted_idx]

plt.figure(figsize=(10, 5))
plt.barh(sorted_features[:10], feature_importance[sorted_idx][:10], color="royalblue")
plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.title("Top 10 Feature Importances (XGBoost)")
plt.gca().invert_yaxis()
plt.show()



import matplotlib.pyplot as plt
import numpy as np

# Get feature importance
feature_importance = xgb_model.feature_importances_
sorted_idx = np.argsort(feature_importance)[::-1]
sorted_features = X_train.columns[sorted_idx]

# Plot feature importance
plt.figure(figsize=(10, 5))
plt.barh(sorted_features[:10], feature_importance[sorted_idx][:10], color="royalblue")
plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.title("Top 10 Feature Importances (XGBoost)")
plt.gca().invert_yaxis()
plt.show()

import joblib
joblib.dump(xgb_model, "xgboost_sales_model.pkl")

"""Model UI Deployment"""

import streamlit as st
import joblib
import numpy as np
import pandas as pd

# Load trained model
model = joblib.load("xgboost_sales_model.pkl")

st.title("Sales Revenue Prediction App")

# User inputs
unit_price = st.number_input("Unit Price ($)", min_value=1)
quantity = st.number_input("Quantity", min_value=1)
shipping_fee = st.number_input("Shipping Fee ($)", min_value=0.0)

# Predict button
if st.button("Predict Revenue"):
    input_data = pd.DataFrame([[unit_price, quantity, shipping_fee]],
                              columns=["Unit Price", "Quantity", "Shipping Fee"])
    prediction = np.expm1(model.predict(input_data))
    st.success(f"Predicted Revenue: ${prediction[0]:,.2f}")

# Re-create the requirements.txt file since the execution state was reset

# Define dependencies based on previous files and imports
requirements_content = """\
pandas
numpy
matplotlib
seaborn
scikit-learn
xgboost
streamlit
joblib
"""

# Save the requirements file
requirements_path = "requirements.txt"
with open(requirements_path, "w") as f:
    f.write(requirements_content)

# Provide the download link
requirements_path